# -*- coding: utf-8 -*-
"""Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rzLhahfYWeUvwfLBZobBV9zzydaqxMZ2

*   Nama Lengkap : Adisaputra Zidha Noorizki
*   Username : hi_zidha
*   Email : hi.zidha@gmail.com
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.optimizers import SGD
from keras.losses import Huber
from keras.callbacks import ModelCheckpoint, EarlyStopping

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IDCamp2023/Intermediate/dataset/householdenergy.csv')

df.head()

df.isnull().sum()

df['DATE'] = pd.to_datetime(df['DATE'])
df['START TIME'] = pd.to_datetime(df['START TIME'])
df['END TIME'] = pd.to_datetime(df['END TIME'])

duration = (df['END TIME'] - df['START TIME']).astype(str).str.split().str[-1]

num_unique_durations = len(duration.unique())
print(f"Number of unique: {num_unique_durations} | {duration.unique()}")

df = df.drop(columns=['TYPE', 'START TIME', 'END TIME', 'UNITS', 'COST', 'NOTES'])
df = df.head(15000)
df

dates = df['DATE'].values
usage = df['USAGE'].values

plt.figure(figsize=(15,5))
plt.plot(dates, usage)
plt.title('Electricity Use',
          fontsize=20)

# Untuk merubah menjadi format yang dapat diterima oleh model
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

window_size = 90
batch_size = 64
buffer = 1000

min_max_scaler = MinMaxScaler()
min_max_scaler.fit(df[['USAGE']])

train_set, test_set = train_test_split(usage, test_size=0.2, shuffle=False)

train_set = windowed_dataset(train_set,
                             window_size=window_size,
                             batch_size=batch_size,
                             shuffle_buffer=buffer)

test_set = windowed_dataset(test_set,
                            window_size=window_size,
                            batch_size=batch_size,
                            shuffle_buffer=buffer)

# Model
model = tf.keras.models.Sequential([
        tf.keras.layers.LSTM(90, return_sequences=True),
        tf.keras.layers.LSTM(60),
        tf.keras.layers.Dense(60, activation="relu"),
        tf.keras.layers.Dense(30, activation="relu"),
        tf.keras.layers.Dense(10, activation="relu"),
        tf.keras.layers.Dense(1),
])

optimizer = SGD(learning_rate=1.0000e-04, momentum=0.8)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# Callbacks
class SaveBestMAEModelCallback(ModelCheckpoint):
  def __init__(self, filepath, monitor='mae', **kwargs):
    super().__init__(filepath, monitor=monitor, save_best_only=True, mode='min', **kwargs)
    self.best_mae = float('inf')

  def on_epoch_end(self, epoch, logs=None):
    current_mae = logs.get(self.monitor)
    if current_mae is not None and current_mae < self.best_mae:
      self.best_mae = current_mae
    super().on_epoch_end(epoch, logs)

num_epochs = 30
checkpoint_path = "bestModel.h5"
save_best_mae_callback = SaveBestMAEModelCallback(checkpoint_path, monitor='mae', verbose=1)

"""MAE (Mean Absolute Error) merupakan metrik evaluasi yang digunakan dalam regresi untuk mengukur selisih antara nilai prediksi dan nilai aktual."""

history = model.fit(train_set, epochs=num_epochs, validation_data=test_set, callbacks=[save_best_mae_callback])

# Plot loss
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Plot MAE (Mean Absolute Error)
plt.figure(figsize=(10,5))
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('MAE (Mean Absolute Error)')
plt.title('Training and Validation MAE')
plt.show()

# 10% of the scale of the data (MAE (Mean Absolute Error))
scale_of_data = df['USAGE'].max() - df['USAGE'].min()
threshold_mae = scale_of_data * 0.10

mae_from_model = round(save_best_mae_callback.best_mae, 3)


if mae_from_model < threshold_mae:
    print(f"MAE from the model: {mae_from_model}, is less than 10% of the scale of the data ({threshold_mae}).")
else:
    print(f"MAE from the model ({mae_from_model}) is greater than or equal to 10% of the scale of the data.")